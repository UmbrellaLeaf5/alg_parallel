# Lesson 5

## MPI – групповые операции (barrier, broadcast и reduce)

### Общие свойства
1. В групповых операциях участвуют все без исключения процессы некоторой группы.
2. MPI-функция, соответствующая групповой операции, должна быть вызвана во всех процессах группы, возможно, в разных частях кода.
3. Выполнение MPI-функции, соответствующей групповой операции, в любом процессе реально начинается только тогда, когда она будет вызвана всеми процессами группы.
4. Если групповая операция связана с явным приемом-передачей информации, то возврат из соответствующей функции происходит только тогда, когда все буферы безопасны для дальнейшего использования.
5. При групповом приеме-передаче информации все процессы должны использовать одну и ту же групповую MPI-функцию.
6. Операции типа точка–точка никак не мешают групповым операциям и не взаимодействуют с ними.
7. В групповых операциях приема-передачи информации отсутствуют атрибуты сообщений (tag), поэтому требуется строго следить за порядком вызова соответствующих функций в коде.
8. Групповые MPI-функции потенциально можно заменить набором вызовов типа точка–точка, однако групповые функции, как правило, работают эффективнее.

### Барьерная синхронизация
Для барьерной синхронизации всех процессов некоторой группы используется функция `MPI_Barrier()`

```C++
int MPI_Barrier(MPI_Comm comm);
```
* `comm` – коммуникатор группы процессов, для которой требуется барьерная синхронизация

### Операция broadcast

Для логарифмической рассылки данных от одного процесса всем
остальным процессам в некоторой группы используется функция
`MPI_Bcast()`.

```C++
int MPI_Bcast(void *buffer, int count, MPI_Datatype datatype, int root, MPI_Comm comm);
```
* `buffer` – для процесса с рангом `root` – адрес памяти, где лежит рассылаемая информация, для остальных – адрес памяти, по которому нужно расположить принятые данные (у каждого процесса может быть свой)
* `count` – количество данных (элементов типа datatype) в сообщении
* `datatype` – тип данных в сообщении
* `root` – ранг процесса в группе, описываемой коммуникатором comm, на котором лежит рассылаемая информация
* `comm` – коммуникатор группы процессов для групповой операции

### Редукционные операции

Редукционные операции – это операции, который обладают свойствами коммутативности `(a op b = b op a)` и ассоциативности `((a op b) op c = a op (b op c))`.

Примеры редукционных операций: `+, *, &, |, &&, ||, max, min,`

Для редукционных операций в некоторой группе процессов используется функция `MPI_Reduce()`.
```C++
int MPI_Reduce(void *sendbuf, void *recvbuf, int count,
MPI_Datatype datatype, MPI_Op op, int root, MPI_Comm comm)
```
* `sendbuf` – адрес памяти, где у процесса лежат исходные данные (у каждого процесса может быть свой)
* `recvbuf` – адрес памяти, куда будет занесен результат, - реально используется только на процессе с рангом `root`
* `op` – код операции (какая редукционная операция выполняется)

|  op  | MPI_OP   |
|------|----------|
| `+`  | MPI_SUM  |
| `*`  | MPI_PROD |
| `min`| MPI_MIN  |
| `max`| MPI_MAX  |
| `&`  | MPI_BAND |
| `or` | MPI_BOR  |
| `&&` | MPI_LAND |
| `or` | MPI_LOR  |

P.S. первое `or` = "|", второе `or` = "||"

* `root` – ранг процесса в группе, описываемой коммуникатором `comm`, на который нужно поместить результат
* `comm` – коммуникатор группы процессов для групповой операции

## Task Pi

Модифицировать программу расчета значения числа пи (`task_2` из `lesson_4`) с использованием `MPI_Bcast` и `MPI_Reduce`.

Для рассылки значения `N` от процесса с рангом 0 использовать `MPI_Bcast`, для окончательного суммирования использовать `MPI_Reduce`.

## MPI-групповые операции (`MPI_Gather`)

### Сбор данных

Для такого сбора информации от всех процессов некоторой группы (на всех процессах одинаковое количество данных) используется функция `MPI_Gather()`.

```C++
int MPI_Gather(void *sendbuf, int sendcount, MPI_Datatype sendtype, void *recvbuf, int recvcount, MPI_Datatype recvtype, int root, MPI_Comm comm);
```
* `sendbuf` – адрес памяти, где у процесса лежат собираемые данные (у каждого процесса может быть свой)
* `sendcount` – количество собираемых данных с процесса (должно быть одинаково для всех процессов)
* `sendtype` – тип собираемых данных (должен быть одинаков для всех процессов)
* `recvbuf` – адрес памяти, начиная с которого будут упорядоченно складываться собираемые данные (использует только root), не может совпадать с `sendbuf`.
* `recvcount` – количество собираемых данных с процесса (использует только root, должно совпадать с `sendcount`)
* `recvtype` – тип собираемых данных (использует только root, должен совпадать с `sendtype`)
* `root` – ранг собирающего процесса
* `comm` – коммуникатор группы процессов

## Task Gather

Есть массив целых чисел из ```N=120``` элементов.

Заполнением массива занимаются различные процессы. Каждый процесс получает в свою зону ответственности одинаковое количество `N / size` элементов и заполняет их значением своего ранга. Если `N` на `size` нацело не делится – прекращаем работу MPI-приложения с сообщением об ошибке.

Далее все собирается на процессе с рангом 0 по порядку с помощью `MPI_Gather` и сбрасывается в отдельный файл в текстовом виде. Значения в выходном файле отделяются друг от друга пробелом.



